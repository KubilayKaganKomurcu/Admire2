# -*- coding: utf-8 -*-
"""finetune_with_peft.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16YjgRzAhnvMRhMBIj6u8wHCbUaeahctM
"""

import pandas as pd
import os
import torch
from PIL import Image
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig
from peft import PeftModel
from qwen_vl_utils import process_vision_info

# ==========================================
# 1. CONFIGURATION (FIXED PATHS)
# ==========================================
# Path to your saved model (Use the one you trained)
ADAPTER_PATH = "Tuggce/best_model_final"
# ADAPTER_PATH = "/content/drive/MyDrive/My_Saved_Model" 

MODEL_ID = "Qwen/Qwen2-VL-2B-Instruct"

#  KEY FIX: Point directly to the file you uploaded
TEST_FILES = [
    {"path": "/content/drive/MyDrive/admire_file/submission_Turkish.tsv", "lang": "TR"},
]


IMAGE_ROOT = "/content/drive/MyDrive/admire2_data/Turkish/"

# ==========================================
# 2. LOAD MODEL
# ==========================================
print("Loading Model...")
bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.float16)
base_model = Qwen2VLForConditionalGeneration.from_pretrained(MODEL_ID, quantization_config=bnb_config, device_map="auto", torch_dtype=torch.float16)

try:
    model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
    print("Adapter loaded successfully!")
except:
    print("WARNING: Could not load adapter. Using Base Model (Predictions will be random).")
    model = base_model

processor = AutoProcessor.from_pretrained(MODEL_ID, min_pixels=256*28*28, max_pixels=512*512)
model.eval()

# ==========================================
# 3. PREDICTION LOOP
# ==========================================
def generate_submission_fixed(file_config):
    results = []

    for entry in file_config:
        path = entry['path']
        lang = entry['lang']
        print(f"\nProcessing {lang} file: {path}")

        # Check if file exists
        if not os.path.exists(path):
            print(f"ERROR: File not found: {path}")
            print("   -> Please upload 'submission_Turkish.tsv' to the Files tab on the left.")
            continue

        # Load Data
        try:
            df = pd.read_csv(path, sep='\t')
        except:
            print(" Failed to read as TSV, trying CSV...")
            df = pd.read_csv(path)

        print(f"  Loaded {len(df)} rows. Starting prediction...")

        for i, row in df.iterrows():
            # Prepare Images
            images = []
            valid_names = []

            # Extract image names from the row
            img_names = [
                row.get('image1_name'), row.get('image2_name'), row.get('image3_name'),
                row.get('image4_name'), row.get('image5_name')
            ]

            # Get Captions if they exist
            captions = [
                row.get('image1_caption', ''), row.get('image2_caption', ''),
                row.get('image3_caption', ''), row.get('image4_caption', ''),
                row.get('image5_caption', '')
            ]

            for fname, cap in zip(img_names, captions):
                fname_str = str(fname).strip()
                # Try to find the image
                # We check a few common locations
                possible_paths = [
                    os.path.join(IMAGE_ROOT, fname_str), # ./images/img.png
                    os.path.join(IMAGE_ROOT, lang, "test", row.get('compound', ''), fname_str), # Drive structure
                    fname_str # Just in the root
                ]

                img_obj = None
                for p in possible_paths:
                    if os.path.exists(p):
                        try:
                            img_obj = Image.open(p).convert("RGB")
                            break
                        except: pass

                if img_obj:
                    images.append(img_obj)
                    valid_names.append(fname_str)
                else:
                    # Fallback: Black Image (So code doesn't crash)
                    images.append(Image.new('RGB', (224, 224), color='black'))
                    valid_names.append(fname_str)

            # Build Prompt
            content_parts = [{"type": "text", "text": "Here are 5 candidate images:\n"}]
            for img, name, cap in zip(images, valid_names, captions):
                cap_text = f"Caption: {str(cap)[:100]}\n" if len(str(cap)) > 5 else ""
                content_parts.append({"type": "text", "text": f"File: {name}\n{cap_text}"})
                content_parts.append({"type": "image", "image": img})

            content_parts.append({"type": "text", "text": (
                f"\nContext Sentence: \"{row['sentence']}\"\n"
                f"Phrase: \"{row['compound']}\"\n"
                f"Task: Rank the filenames based on how well the image matches the meaning of the phrase in this context.\n"
                f"Output strictly the comma-separated list of filenames."
            )})

            messages = [{"role": "user", "content": content_parts}]

            # Inference
            text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            image_inputs, video_inputs = process_vision_info(messages)
            inputs = processor(text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors="pt").to("cuda")

            with torch.no_grad():
                generated_ids = model.generate(**inputs, max_new_tokens=100)

            output_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
            raw_output = output_text.split("assistant")[-1] if "assistant" in output_text else output_text

            # Simple Cleaner
            cleaned = raw_output.replace("Ranking:", "").replace("File:", "").strip()

            results.append({
                "lang": lang,
                "compound": row['compound'],
                "sentence": row['sentence'],
                "predicted_ranking": cleaned
            })

            if i > 0 and i % 10 == 0: print(f"   Processed {i}/{len(df)}...")

    # Save Results
    if len(results) > 0:
        out_file = "submission_Turkish_predictions.csv"
        pd.DataFrame(results).to_csv(out_file, index=False)
        print(f"\n✅ SUCCESS: Saved {len(results)} predictions to '{out_file}'")
    else:
        print("\n❌ ERROR: No results were generated. Check your file paths.")

# Run it
generate_submission_fixed(TEST_FILES)
